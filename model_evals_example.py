import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers
from matplotlib import pyplot as plt
from matplotlib import rcParams
import seaborn as sns
from google.colab import widgets
# For facets
from IPython.core.display import display, HTML
import base64
!pip install facets-overview==1.0.0
from facets_overview.feature_statistics_generator import FeatureStatisticsGenerator

# The following lines adjust the granularity of reporting.
pd.options.display.max_rows = 10
pd.options.display.float_format = "{:.1f}".format

COLUMNS = ["age", "workclass", "fnlwgt", "education", "education_num",
          "marital_status", "occupation", "relationship", "race", "gender",
          "capital_gain", "capital_loss", "hours_per_week", "native_country",
          "income_bracket"]

train_csv = tf.keras.utils.get_file('adult.data',
 'https://download.mlcc.google.com/mledu-datasets/adult_census_train.csv')
test_csv = tf.keras.utils.get_file('adult.test' ,
 'https://download.mlcc.google.com/mledu-datasets/adult_census_test.csv')
train_df = pd.read_csv(train_csv, names=COLUMNS, sep=r'\s*,\s*',
                      engine='python', na_values="?")
test_df = pd.read_csv(test_csv, names=COLUMNS, sep=r'\s*,\s*', skiprows=[0],
                     engine='python', na_values="?")
# Strip trailing periods mistakenly included only in UCI test dataset.
test_df['income_bracket'] = test_df.income_bracket.str.rstrip('.')
fsg = FeatureStatisticsGenerator()
dataframes = [
   {'table': train_df, 'name': 'trainData'}]
censusProto = fsg.ProtoFromDataFrames(dataframes)
protostr = base64.b64encode(censusProto.SerializeToString()).decode("utf-8")
HTML_TEMPLATE = """<script src="https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js"></script>
       <link rel="import" href="https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html">
       <facets-overview id="elem"></facets-overview>
       <script>
         document.querySelector("#elem").protoInput = "{protostr}";
       </script>"""
html = HTML_TEMPLATE.format(protostr=protostr)
display(HTML(html))
SAMPLE_SIZE = 5000
train_dive = train_df.sample(SAMPLE_SIZE).to_json(orient='records')
HTML_TEMPLATE = """<script src="https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js"></script>
       <link rel="import" href="https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html">
       <facets-dive id="elem" height="600"></facets-dive>
       <script>
         var data = {jsonstr};
         document.querySelector("#elem").data = data;
       </script>"""
html = HTML_TEMPLATE.format(jsonstr=train_dive)
display(HTML(html))


def pandas_to_numpy(data):
 '''Convert a pandas DataFrame into a Numpy array'''


 # Drop empty rows.
 data = data.dropna(how="any", axis=0)
 # Separate DataFrame into two Numpy arrays.
 labels = np.array(data['income_bracket'] == ">50K")
 features = data.drop('income_bracket', axis=1)
 features = {name:np.array(value) for name, value in features.items()}
 return features, labels

# Since we don't know the full range of possible values with occupation and
# native_country, we'll use categorical_column_with_hash_bucket() to help map
# each feature string into an integer ID.
occupation = tf.feature_column.categorical_column_with_hash_bucket(
   "occupation", hash_bucket_size=1000)
native_country = tf.feature_column.categorical_column_with_hash_bucket(
   "native_country", hash_bucket_size=1000)
# For the remaining categorical features, since we know what the possible values
# are, we can be more explicit and use categorical_column_with_vocabulary_list()
gender = tf.feature_column.categorical_column_with_vocabulary_list(
   "gender", ["Female", "Male"])
race = tf.feature_column.categorical_column_with_vocabulary_list(
   "race", [
       "White", "Asian-Pac-Islander", "Amer-Indian-Eskimo", "Other", "Black"
   ])
education = tf.feature_column.categorical_column_with_vocabulary_list(
   "education", [
       "Bachelors", "HS-grad", "11th", "Masters", "9th",
       "Some-college", "Assoc-acdm", "Assoc-voc", "7th-8th",
       "Doctorate", "Prof-school", "5th-6th", "10th", "1st-4th",
       "Preschool", "12th"
   ])
marital_status = tf.feature_column.categorical_column_with_vocabulary_list(
   "marital_status", [
       "Married-civ-spouse", "Divorced", "Married-spouse-absent",
       "Never-married", "Separated", "Married-AF-spouse", "Widowed"
   ])
relationship = tf.feature_column.categorical_column_with_vocabulary_list(
   "relationship", [
       "Husband", "Not-in-family", "Wife", "Own-child", "Unmarried",
       "Other-relative"
   ])
workclass = tf.feature_column.categorical_column_with_vocabulary_list(
   "workclass", [
       "Self-emp-not-inc", "Private", "State-gov", "Federal-gov",
       "Local-gov", "?", "Self-emp-inc", "Without-pay", "Never-worked"
   ])
# For Numeric features, we can just call on feature_column.numeric_column()
# to use its raw value instead of having to create a map between value and ID.
age = tf.feature_column.numeric_column("age")
fnlwgt = tf.feature_column.numeric_column("fnlwgt")
education_num = tf.feature_column.numeric_column("education_num")
capital_gain = tf.feature_column.numeric_column("capital_gain")
capital_loss = tf.feature_column.numeric_column("capital_loss")
hours_per_week = tf.feature_column.numeric_column("hours_per_week")
age_buckets = tf.feature_column.bucketized_column(
   age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])
# List of variables, with special handling for gender subgroup.
variables = [native_country, education, occupation, workclass,
            relationship, age_buckets]
subgroup_variables = [gender]
feature_columns = variables + subgroup_variables
deep_columns = [
   tf.feature_column.indicator_column(workclass),
   tf.feature_column.indicator_column(education),
   tf.feature_column.indicator_column(age_buckets),
   tf.feature_column.indicator_column(relationship),
   tf.feature_column.embedding_column(native_country, dimension=8),
   tf.feature_column.embedding_column(occupation, dimension=8),
]
# Parameters
HIDDEN_UNITS_LAYER_01 = 128
HIDDEN_UNITS_LAYER_02 = 64
LEARNING_RATE = 0.1
L1_REGULARIZATION_STRENGTH = 0.001
L2_REGULARIZATION_STRENGTH = 0.001
RANDOM_SEED = 512
tf.random.set_seed(RANDOM_SEED)
# List of built-in metrics that we'll need to evaluate performance.
METRICS = [
 tf.keras.metrics.TruePositives(name='tp'),
 tf.keras.metrics.FalsePositives(name='fp'),
 tf.keras.metrics.TrueNegatives(name='tn'),
 tf.keras.metrics.FalseNegatives(name='fn'),
 tf.keras.metrics.BinaryAccuracy(name='accuracy'),
 tf.keras.metrics.Precision(name='precision'),
 tf.keras.metrics.Recall(name='recall'),
 tf.keras.metrics.AUC(name='auc'),
]
regularizer = tf.keras.regularizers.l1_l2(
   l1=L1_REGULARIZATION_STRENGTH, l2=L2_REGULARIZATION_STRENGTH)
model = tf.keras.Sequential([
 layers.DenseFeatures(deep_columns),
 layers.Dense(
     HIDDEN_UNITS_LAYER_01, activation='relu', kernel_regularizer=regularizer),
 layers.Dense(
     HIDDEN_UNITS_LAYER_02, activation='relu', kernel_regularizer=regularizer),
 layers.Dense(
     1, activation='sigmoid', kernel_regularizer=regularizer)
])
model.compile(optimizer=tf.keras.optimizers.Adagrad(LEARNING_RATE), 
             loss=tf.keras.losses.BinaryCrossentropy(),
             metrics=METRICS)
features, labels = pandas_to_numpy(test_df)
model.evaluate(x=features, y=labels);


def plot_confusion_matrix(
   confusion_matrix, class_names, subgroup, figsize = (8,6)):
 # We're taking our calculated binary confusion matrix that's already in the
 # form of an array and turning it into a pandas DataFrame because it's a lot
 # easier to work with a pandas DataFrame when visualizing a heat map in
 # Seaborn.
 df_cm = pd.DataFrame(
     confusion_matrix, index=class_names, columns=class_names,
 )
 rcParams.update({
 'font.family':'sans-serif',
 'font.sans-serif':['Liberation Sans'],
 })
 sns.set_context("notebook", font_scale=1.25)
 fig = plt.figure(figsize=figsize)
 plt.title('Confusion Matrix for Performance Across ' + subgroup)
 # Combine the instance (numercial value) with its description
 strings = np.asarray([['True Positives', 'False Negatives'],
                       ['False Positives', 'True Negatives']])
 labels = (np.asarray(
     ["{0:g}\n{1}".format(value, string) for string, value in zip(
         strings.flatten(), confusion_matrix.flatten())])).reshape(2, 2)
 heatmap = sns.heatmap(df_cm, annot=labels, fmt="",
     linewidths=2.0, cmap=sns.color_palette("GnBu_d"));
 heatmap.yaxis.set_ticklabels(
     heatmap.yaxis.get_ticklabels(), rotation=0, ha='right')
 heatmap.xaxis.set_ticklabels(
     heatmap.xaxis.get_ticklabels(), rotation=45, ha='right')
 plt.ylabel('References')
 plt.xlabel('Predictions')
 return fig


CATEGORY  =  "gender"
SUBGROUP =  "Male"
# Labels for annotating axes in plot.
classes = ['Over $50K', 'Less than $50K']
# Given define subgroup, generate predictions and obtain its corresponding
# ground truth.
subgroup_filter  = test_df.loc[test_df[CATEGORY] == SUBGROUP]
features, labels = pandas_to_numpy(subgroup_filter)
subgroup_results = model.evaluate(x=features, y=labels, verbose=0)
confusion_matrix = np.array([[subgroup_results[1], subgroup_results[4]],
                            [subgroup_results[2], subgroup_results[3]]])
subgroup_performance_metrics = {
   'ACCURACY': subgroup_results[5],
   'PRECISION': subgroup_results[6],
   'RECALL': subgroup_results[7],
   'AUC': subgroup_results[8]
}
performance_df = pd.DataFrame(subgroup_performance_metrics, index=[SUBGROUP])
pd.options.display.float_format = '{:,.4f}'.format
plot_confusion_matrix(confusion_matrix, classes, SUBGROUP);
performance_df
